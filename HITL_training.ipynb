{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d8d450f-8f59-4ee1-a531-9a7306e5b950",
   "metadata": {},
   "source": [
    "load necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be562f7a-5f83-4a7d-81a6-ef8b9e0b67ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RWKV_JIT_ON 1 RWKV_CUDA_ON 0 RESCALE_LAYER 999\n",
      "\n",
      "Loading check_points/finetuned/lora_out_0.pth ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/bruce/ssd41/zhejing/infilling/code/upload/model_inference.py:209: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.w = torch.load(args.MODEL_NAME, map_location='cpu') # load model to CPU first\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model detected: v4.0\n",
      "Strategy: (total 64+1=65 layers)\n",
      "* cuda:0 [float16, float16], store 65 layers\n",
      "* cpu fp32, store 0 layers\n",
      "0-cuda:0-float16-float16 1-cuda:0-float16-float16 2-cuda:0-float16-float16 3-cuda:0-float16-float16 4-cuda:0-float16-float16 5-cuda:0-float16-float16 6-cuda:0-float16-float16 7-cuda:0-float16-float16 8-cuda:0-float16-float16 9-cuda:0-float16-float16 10-cuda:0-float16-float16 11-cuda:0-float16-float16 12-cuda:0-float16-float16 13-cuda:0-float16-float16 14-cuda:0-float16-float16 15-cuda:0-float16-float16 16-cuda:0-float16-float16 17-cuda:0-float16-float16 18-cuda:0-float16-float16 19-cuda:0-float16-float16 20-cuda:0-float16-float16 21-cuda:0-float16-float16 22-cuda:0-float16-float16 23-cuda:0-float16-float16 24-cuda:0-float16-float16 25-cuda:0-float16-float16 26-cuda:0-float16-float16 27-cuda:0-float16-float16 28-cuda:0-float16-float16 29-cuda:0-float16-float16 30-cuda:0-float16-float16 31-cuda:0-float16-float16 32-cuda:0-float16-float16 33-cuda:0-float16-float16 34-cuda:0-float16-float16 35-cuda:0-float16-float16 36-cuda:0-float16-float16 37-cuda:0-float16-float16 38-cuda:0-float16-float16 39-cuda:0-float16-float16 40-cuda:0-float16-float16 41-cuda:0-float16-float16 42-cuda:0-float16-float16 43-cuda:0-float16-float16 44-cuda:0-float16-float16 45-cuda:0-float16-float16 46-cuda:0-float16-float16 47-cuda:0-float16-float16 48-cuda:0-float16-float16 49-cuda:0-float16-float16 50-cuda:0-float16-float16 51-cuda:0-float16-float16 52-cuda:0-float16-float16 53-cuda:0-float16-float16 54-cuda:0-float16-float16 55-cuda:0-float16-float16 56-cuda:0-float16-float16 57-cuda:0-float16-float16 58-cuda:0-float16-float16 59-cuda:0-float16-float16 60-cuda:0-float16-float16 61-cuda:0-float16-float16 62-cuda:0-float16-float16 63-cuda:0-float16-float16 64-cuda:0-float16-float16 \n",
      "emb.weight                        f16      cpu  20099   800 \n",
      "blocks.0.ln1.weight               f16   cuda:0    800       \n",
      "blocks.0.ln1.bias                 f16   cuda:0    800       \n",
      "blocks.0.ln2.weight               f16   cuda:0    800       \n",
      "blocks.0.ln2.bias                 f16   cuda:0    800       \n",
      "blocks.0.att.time_decay           f32   cuda:0    800       \n",
      "blocks.0.att.time_first           f32   cuda:0    800       \n",
      "blocks.0.att.time_mix_k           f16   cuda:0    800       \n",
      "blocks.0.att.time_mix_v           f16   cuda:0    800       \n",
      "blocks.0.att.time_mix_r           f16   cuda:0    800       \n",
      "blocks.0.att.key.weight           f16   cuda:0    800   800 \n",
      "blocks.0.att.value.weight         f16   cuda:0    800   800 \n",
      "blocks.0.att.receptance.weight    f16   cuda:0    800   800 \n",
      "blocks.0.att.output.weight        f16   cuda:0    800   800 \n",
      "blocks.0.ffn.time_mix_k           f16   cuda:0    800       \n",
      "blocks.0.ffn.time_mix_r           f16   cuda:0    800       \n",
      "blocks.0.ffn.key.weight           f16   cuda:0    800  3200 \n",
      "blocks.0.ffn.receptance.weight    f16   cuda:0    800   800 \n",
      "blocks.0.ffn.value.weight         f16   cuda:0   3200   800 \n",
      "............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "blocks.63.ln1.weight              f16   cuda:0    800       \n",
      "blocks.63.ln1.bias                f16   cuda:0    800       \n",
      "blocks.63.ln2.weight              f16   cuda:0    800       \n",
      "blocks.63.ln2.bias                f16   cuda:0    800       \n",
      "blocks.63.att.time_decay          f32   cuda:0    800       \n",
      "blocks.63.att.time_first          f32   cuda:0    800       \n",
      "blocks.63.att.time_mix_k          f16   cuda:0    800       \n",
      "blocks.63.att.time_mix_v          f16   cuda:0    800       \n",
      "blocks.63.att.time_mix_r          f16   cuda:0    800       \n",
      "blocks.63.att.key.weight          f16   cuda:0    800   800 \n",
      "blocks.63.att.value.weight        f16   cuda:0    800   800 \n",
      "blocks.63.att.receptance.weight   f16   cuda:0    800   800 \n",
      "blocks.63.att.output.weight       f16   cuda:0    800   800 \n",
      "blocks.63.ffn.time_mix_k          f16   cuda:0    800       \n",
      "blocks.63.ffn.time_mix_r          f16   cuda:0    800       \n",
      "blocks.63.ffn.key.weight          f16   cuda:0    800  3200 \n",
      "blocks.63.ffn.receptance.weight   f16   cuda:0    800   800 \n",
      "blocks.63.ffn.value.weight        f16   cuda:0   3200   800 \n",
      "ln_out.weight                     f16   cuda:0    800       \n",
      "ln_out.bias                       f16   cuda:0    800       \n",
      "ln_concat.weight                  f16   cuda:0   1600       \n",
      "ln_concat.bias                    f16   cuda:0   1600       \n",
      "head.weight                       f16   cuda:0    800 20099 \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "import subprocess\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from rwkv.utils import PIPELINE\n",
    "\n",
    "from MIDI import midi_util\n",
    "from MIDI.midi_util import VocabConfig,FilterConfig\n",
    "\n",
    "from model_mjepa import Encoder, Predictor, apply_masks\n",
    "from model_inference import RWKV\n",
    "from src.dataset import MyDataset\n",
    "from src.dataset import process_sequence_array\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['RWKV_JIT_ON'] = '1'\n",
    "# os.environ[\"RWKV_CUDA_ON\"] = '1'\n",
    "os.environ[\"RWKV_RESCALE_LAYER\"] = '999'\n",
    "\n",
    "\n",
    "model_path='check_points/finetuned/lora_out_0.pth'\n",
    "\n",
    "\n",
    "MODEL_FILE = None\n",
    "\n",
    "EOS_ID = 0\n",
    "TOKEN_SEP = ' '\n",
    "TRIAL = 0\n",
    "ANS_ID=20098\n",
    "\n",
    "\n",
    "# Load model and tokenizer\n",
    "# model = RWKV(model=model_path, strategy='cpu fp32')\n",
    "model = RWKV(model=model_path, strategy='cuda:0 fp16 *65 -> cpu fp32')\n",
    "\n",
    "pipeline = PIPELINE(model, \"json2binidx_tool/tools/tokenizer-midi.json\")\n",
    "tokenizer = pipeline\n",
    "vocab_config_path=\"MIDI/vocab_config.json\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a5dbd9d-11cb-41bd-8b40-bc1c3518faf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def text_to_midi(input_text, output_path, vocab_config_path,bpm):\n",
    "    cfg = VocabConfig.from_json(vocab_config_path)\n",
    "    text = input_text.strip()\n",
    "    mid = midi_util.convert_str_to_midi(cfg, text,bpm)\n",
    "    mid.save(os.path.abspath(output_path))\n",
    "\n",
    "def forward_target(data,masks_pred):\n",
    "    with torch.no_grad():\n",
    "        h = jepa_target_encoder(data)\n",
    "        h = F.layer_norm(h, (h.size(-1),))  # normalize over feature-dim  #1 4094 512\n",
    "        h = apply_masks(h, masks_pred)   # keep masks_pred\n",
    "        # h = h.mean(dim=1, keepdim=True)\n",
    "        return h\n",
    "def filter_context(a,b):\n",
    "    # Ensure b is a 1D tensor for easier comparison\n",
    "    b_flat = b.view(-1)\n",
    "    \n",
    "    # Initialize a mask of zeros with the same length as 'a'\n",
    "    # We'll fill this with ones for elements that should be kept\n",
    "    mask = torch.ones_like(a, dtype=torch.bool)\n",
    "    \n",
    "    # Iterate through each element of 'b_flat' and update the mask\n",
    "    # Set mask to False for elements in 'a' that are found in 'b_flat'\n",
    "    for val in b_flat:\n",
    "        mask &= (a != val)\n",
    "    \n",
    "    # Apply the mask to 'a' to filter out unwanted elements\n",
    "    filtered_a = a[mask]\n",
    "    \n",
    "    # Reshape filtered_a back to the original shape if necessary\n",
    "    filtered_a = filtered_a.view(1, -1)\n",
    "    \n",
    "    # Print the result\n",
    "    return filtered_a\n",
    "\n",
    "def generate(z1):\n",
    "    ccc = list(final_sequence[0][:np.where(final_sequence[0]==20097)[0][0]+1])\n",
    "    gt= list(final_sequence[0][np.where(final_sequence[0]==20097)[0][0]+1:np.where(final_sequence[0]==20098)[0][0]])\n",
    "    \n",
    "    occurrence = {}\n",
    "    state = None\n",
    "    ccc1=[]\n",
    "    for i in range(args.ctx_len):\n",
    "        if i == 0:\n",
    "            out, state = model.forward(ccc, state,z1)\n",
    "        else:\n",
    "            out, state = model.forward([token], state,z1)\n",
    "    \n",
    "        # MIDI mode adjustments\n",
    "        for n in occurrence:\n",
    "            out[n] -= (0 + occurrence[n] * 0.5)\n",
    "    \n",
    "        out[0] += (i - 2000/2) / (500/2)  # not too short, not too long\n",
    "        if args.patch_size>1000:\n",
    "            out[20098] += (i - 1000/1) / (500/1)  # not too short, not too long\n",
    "        else:\n",
    "            out[20098] += (i - 1000/4) / (500/4)  # not too short, not too long    \n",
    "        out[127] -= 1  # avoid \"t125\"\n",
    "        out[20097] -= 1  \n",
    "        out[20096] -= 1  \n",
    "        token = pipeline.sample_logits(out, temperature=1.0, top_k=8, top_p=0.8)\n",
    "        if token == ANS_ID: \n",
    "            print('finish!!!') \n",
    "            break\n",
    "        if token == EOS_ID: break    \n",
    "        for n in occurrence: occurrence[n] *= 0.997  #### decay repetition penalty\n",
    "        if token >= 128 or token == 127:\n",
    "            occurrence[token] = 1 + (occurrence[token] if token in occurrence else 0)\n",
    "        else:\n",
    "            occurrence[token] = 0.3 + (occurrence[token] if token in occurrence else 0)\n",
    "        ccc += [token]\n",
    "        ccc1+=[token]\n",
    "    ccc+= [ANS_ID]\n",
    "    idx=np.where(np.array(ccc)==20096)[0][0] #mask\n",
    "    idx1=np.where(np.array(ccc)==20097)[0][0] #sep\n",
    "    new_seq=ccc[:idx]+ccc1+ccc[idx+1:idx1]\n",
    "    gt_new_seq=ccc[:idx]+gt+ccc[idx+1:idx1]\n",
    "    patched_new=[list(range(len(ccc[:idx]),(len(ccc[:idx])+len(ccc1))))]\n",
    "    non_patched_new=[list(range(len(ccc[:idx]))) + list(range(patched_new[0][-1]+1,len(new_seq)))]\n",
    "    masks_pred_new=torch.tensor(np.array(patched_new))\n",
    "    masks_enc_new=torch.tensor(np.array(non_patched_new))\n",
    "    seq='<start>'\n",
    "    for c in new_seq:\n",
    "        seq+=' '+tokenizer.decode([c])\n",
    "    seq+=' <end>'\n",
    "    \n",
    "    \n",
    "    gt_seq='<start>'\n",
    "    for c in gt_new_seq:\n",
    "        gt_seq+=' '+tokenizer.decode([c])\n",
    "    gt_seq+=' <end>'\n",
    "    data_to_save = np.array([ccc[:idx], ccc1, gt, ccc[idx+1:idx1],tokenizer.encode(seq)], dtype=object)\n",
    "    text_to_midi(seq,'preference/'+str(j)+'_'+str(ite)+'.mid',\"./MIDI/vocab_config.json\",120)\n",
    "    text_to_midi(gt_seq,'preference/gt_'+str(j)+'_'+str(ite)+'.mid',\"./MIDI/vocab_config.json\",120)\n",
    "    return masks_pred_new, masks_enc_new,torch.tensor(new_seq[:])\n",
    "def create_answer_sequence_array(sequence, start_indices, patch_size, sep_token=20098):\n",
    "    \"\"\"\n",
    "    Create the answer sequence from selected patches, separated by `sep_token` using NumPy.\n",
    "    \"\"\"\n",
    "    answer = []\n",
    "    for start_idx in start_indices:\n",
    "        patch = sequence[start_idx:start_idx + patch_size]\n",
    "        answer.extend(patch)\n",
    "        answer.append(sep_token)\n",
    "    # Remove the last sep_token\n",
    "    answer = answer[:]\n",
    "    return np.array(answer)\n",
    "\n",
    "\n",
    "\n",
    "def get_non_patched_indices(sequence_length, patched_indices):\n",
    "    \"\"\"\n",
    "    Determine non-patched indices from the sequence based on patched indices.\n",
    "    \"\"\"\n",
    "    non_patched_indices = [i for i in range(sequence_length) if i not in patched_indices]\n",
    "    return non_patched_indices\n",
    "\n",
    "\n",
    "def process_sequence_array(sequence, num_patches=1, ctx_len=4096, mask_token=20096, sep_token=20097, ans_token=20098):\n",
    "    \"\"\"\n",
    "    Apply modifications to a single sequence according to specified rules using NumPy.\n",
    "    Returns the final modified sequence along with patched and non-patched indices.\n",
    "    \"\"\"\n",
    "\n",
    "    patch_size=args.patch_size\n",
    "    patched_indices_all=[]\n",
    "    non_patched_indices_all=[]\n",
    "    final_sequence_all=[]\n",
    "    for i in range(len(sequence)):\n",
    "        if len(np.where(sequence[i]==0)[0])==0:\n",
    "            non_zero=ctx_len-3\n",
    "        else:\n",
    "            non_zero=np.where(sequence[i]==0)[0][0]\n",
    "        # print('non_zero:',non_zero)\n",
    "        # patch_size = np.random.randint(16, min(non_zero-3,int(ctx_len*0.4)))\n",
    "        available_index=non_zero-patch_size-1\n",
    "        if args.fixed_location==1:\n",
    "            start_indices = 128\n",
    "        else:\n",
    "            start_indices = np.random.choice(range(1,available_index))\n",
    "\n",
    "        patched_indices=list(range(start_indices,start_indices+patch_size))\n",
    "        non_patched_indices=get_non_patched_indices(len(sequence[i]), patched_indices)\n",
    "        answer_seq = create_answer_sequence_array(sequence[i], [start_indices], patch_size, sep_token=20098)\n",
    "        new_sequence=torch.concat([sequence[i][:start_indices],torch.tensor([20096]),sequence[i][start_indices+patch_size:non_zero-1],\n",
    "             torch.tensor([20097]),torch.tensor(answer_seq),torch.tensor([2])])\n",
    "\n",
    "        if len(np.where(sequence[i]==0)[0])!=0:\n",
    "            final_sequence=np.array(torch.concat([new_sequence,torch.zeros(4096-new_sequence.shape[0],dtype=int)]))\n",
    "        else:\n",
    "            final_sequence = np.array(new_sequence)\n",
    "        # print('tessas!',final_sequence.shape)\n",
    "        final_sequence_all.append(final_sequence)\n",
    "        patched_indices_all.append(patched_indices)\n",
    "        non_patched_indices_all.append(non_patched_indices)\n",
    "\n",
    "    return final_sequence_all, patched_indices_all, non_patched_indices_all\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d753759f-0f0c-42ef-8291-4e9c87998253",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current vocab size = 20099 (make sure it's correct)\n",
      "Train Data has 909 samples longer than 512 tokens.\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        # Existing settings\n",
    "        self.proj_dir = \"out\"\n",
    "        self.data_file = \"data/pop909_document\"\n",
    "        self.data_type = \"binidx\"\n",
    "        self.vocab_size = 20099\n",
    "        self.ctx_len = 4096\n",
    "        self.epoch_steps = 100\n",
    "        self.micro_bsz = 4\n",
    "        self.embed_dim = 512\n",
    "        self.encoder_depth=6\n",
    "        self.predictor_embed_dim=256\n",
    "        self.predictor_depth=3\n",
    "        self.predictor_num_heads=4\n",
    "        self.encoder_num_heads=4      \n",
    "        self.mlp_ratio = 4\n",
    "        self.drop_rate=0.1\n",
    "        self.norm_layer=nn.LayerNorm\n",
    "        self.init_std=0.02\n",
    "        self.device= 'cuda:1'\n",
    "        self.ipe_scale = 1\n",
    "        self.ema = (0.996, 1.0)\n",
    "        # New settings from provided args\n",
    "        self.batch_size = self.micro_bsz\n",
    "        self.num_workers = 10\n",
    "        self.pin_mem = True\n",
    "        self.patch_number= 1\n",
    "\n",
    "        # Optimization\n",
    "        self.epochs = 300\n",
    "        self.final_lr = 1e-6\n",
    "        self.final_weight_decay = 0.4\n",
    "        self.lr = 0.001\n",
    "        self.start_lr = 0.0002\n",
    "        self.warmup = 40\n",
    "        self.weight_decay = 0.04\n",
    "        self.fixed_location=1\n",
    "        self.patch_size=128\n",
    "        \n",
    "args = Args()\n",
    "train_data = MyDataset(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2349fa30-3727-43e0-bb58-b613d80f87c5",
   "metadata": {},
   "source": [
    "load pre-trained MJEPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8cab793-f0b7-427c-ad52-6f8d8c792545",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Predictor(\n",
       "  (predictor_embed): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (predictor_pos_embed): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (predictor_blocks): ModuleList(\n",
       "    (0-2): 3 x TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (predictor_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  (predictor_proj): Linear(in_features=256, out_features=512, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "jepa_encoder = Encoder(args)\n",
    "jepa_predictor = Predictor(args)\n",
    "jepa_target_encoder= Encoder(args)\n",
    "\n",
    "checkpoint = torch.load('./check_points/mjepa/jepa-latest.pth.tar', map_location=torch.device(args.device),weights_only=True)\n",
    "\n",
    "pretrained_dict = checkpoint['encoder']\n",
    "jepa_encoder.load_state_dict(pretrained_dict)\n",
    "pretrained_dict = checkpoint['predictor']\n",
    "jepa_predictor.load_state_dict(pretrained_dict)\n",
    "\n",
    "\n",
    "pretrained_dict = checkpoint['target_encoder']\n",
    "jepa_target_encoder.load_state_dict(pretrained_dict)\n",
    "\n",
    "jepa_target_encoder.eval()\n",
    "jepa_encoder.eval()\n",
    "jepa_predictor.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f42718d-c4a4-4794-a730-31ccc6d707af",
   "metadata": {},
   "source": [
    "HITL finetuning MJEPA based on like and dislike parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d38c4524-34c3-46a5-a0d9-4371967ed54d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: Music 102 0\n",
      "Start: Iteration: 0 0\n",
      "0 0 start !!!\n",
      "INFO:root:Using AdamW\n",
      "INFO:root:loaded pretrained encoder from epoch 10 with msg: <All keys matched successfully>\n",
      "INFO:root:loaded pretrained encoder from epoch 10 with msg: <All keys matched successfully>\n",
      "['encoder', 'predictor', 'target_encoder', 'opt', 'scaler', 'epoch', 'loss', 'batch_size', 'world_size', 'lr']\n",
      "INFO:root:loaded pretrained encoder from epoch 10 with msg: <All keys matched successfully>\n",
      "INFO:root:loaded optimizers from epoch 10\n",
      "INFO:root:read-path: ./check_points/mjepa/jepa-latest.pth.tar\n",
      "INFO:root:[1,     1] loss: 0.198 pos and neg: 3.7 3.6 [wd: 4.00e-01] [lr: 1.45e-05] [mem: 2.68e+03] (1207.1 ms)\n",
      "INFO:root:[1,     1] grad_stats: [0.00e+00 0.00e+00] (1.83e-01, 4.14e+00)\n",
      "INFO:root:avg. loss 0.198\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2162089/620473683.py:39: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('./preference/jepa-latest.pth.tar', map_location=torch.device(args.device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish!!!\n",
      "INFO:root:Using AdamW\n",
      "INFO:root:loaded pretrained encoder from epoch 1 with msg: <All keys matched successfully>\n",
      "INFO:root:loaded pretrained encoder from epoch 1 with msg: <All keys matched successfully>\n",
      "['encoder', 'predictor', 'target_encoder', 'opt', 'scaler', 'epoch', 'loss', 'batch_size', 'world_size', 'lr']\n",
      "INFO:root:loaded pretrained encoder from epoch 1 with msg: <All keys matched successfully>\n",
      "INFO:root:loaded optimizers from epoch 1\n",
      "INFO:root:read-path: ./preference/jepa-latest.pth.tar\n",
      "INFO:root:[1,     1] loss: 7.542 pos and neg: 4.1 3.5 [wd: 4.00e-01] [lr: 1.45e-05] [mem: 1.75e+03] (1048.8 ms)\n",
      "INFO:root:[1,     1] grad_stats: [0.00e+00 0.00e+00] (1.18e+00, 9.95e+01)\n",
      "INFO:root:avg. loss 7.542\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jepa_target_encoder=jepa_target_encoder.to(args.device)\n",
    "lis = np.random.randint(0, 900, 1)\n",
    "for j in range(len(lis)):\n",
    "    print('Start: Music', lis[j],j)\n",
    "    a=train_data.__getitem__(lis[j])  # original piece   # no indicator tokens\n",
    "    final_sequence,patched,non_patched=process_sequence_array(a.unsqueeze(0),args.patch_number,len(a)) #randomly initiate dislike part\n",
    "    masks_pred=torch.tensor(np.array(patched))\n",
    "    masks_context=torch.tensor(np.array(non_patched))\n",
    "    with open('./preference/dislike', 'wb') as f:\n",
    "        pickle.dump([a.unsqueeze(0),masks_pred,masks_context], f)        \n",
    "    \n",
    "    for ite in range(1):\n",
    "        print('Start: Iteration:',j, ite)\n",
    "        #step 1: select like and dislike\n",
    "        masks_pred_dislike =masks_pred    \n",
    "        like_length=min(masks_pred_dislike.shape[1], masks_context.shape[1])            \n",
    "        like_idx=np.random.choice(masks_context.shape[1]- like_length)\n",
    "        masks_pred_like=masks_context[0][like_idx:like_idx+like_length].unsqueeze(0)\n",
    "        masks_context_dislike=filter_context(masks_context,masks_pred_like)\n",
    "        masks_context_like=masks_context_dislike\n",
    "        ll=[a.unsqueeze(0),masks_pred_like,masks_pred_dislike,masks_context_like,masks_context_dislike]\n",
    "        with open('./preference/samples', 'wb') as f:\n",
    "            pickle.dump(ll, f)\n",
    "        #step2 finetune JEPA based on like and dislike; iteration-wise\n",
    "        epochs = 1\n",
    "        file_path= \"./preference/samples\"\n",
    "        log_path= \"./preference/\"\n",
    "        if j==0 and ite==0: #intiate mjepa\n",
    "            print(j,ite,'start !!!')\n",
    "            load_checkpoint='./check_points/mjepa/jepa-latest.pth.tar'\n",
    "        else:\n",
    "            load_checkpoint='./preference/jepa-latest.pth.tar'\n",
    "        command = f\"torchrun --nproc_per_node=1 train_personalization_iteration.py --load_checkpoint {load_checkpoint} --epochs {epochs} --file_path {file_path} --log_folder {log_path}\"\n",
    "        process = subprocess.run(command.split(), capture_output=True, text=True)\n",
    "        # Output the results\n",
    "        print(process.stdout)\n",
    "        print(process.stderr)\n",
    "        #step3 reload JEPA encoder and generate new music\n",
    "        checkpoint = torch.load('./preference/jepa-latest.pth.tar', map_location=torch.device(args.device))\n",
    "        jepa_encoder.load_state_dict(checkpoint['encoder'])\n",
    "        jepa_predictor.load_state_dict(checkpoint['predictor'])\n",
    "        jepa_target_encoder.load_state_dict(checkpoint['target_encoder'])\n",
    "        jepa_encoder=jepa_encoder.to(args.device)\n",
    "        jepa_target_encoder=jepa_target_encoder.to(args.device)\n",
    "        jepa_predictor=jepa_predictor.to(args.device)\n",
    "        jepa_encoder.eval()\n",
    "        jepa_target_encoder.eval()\n",
    "        jepa_predictor.eval()\n",
    "        a=a.to(args.device)\n",
    "        z = jepa_encoder(a.unsqueeze(0), masks_context_dislike.to(args.device))\n",
    "        z1 = jepa_predictor(z, a.unsqueeze(0), masks_context_dislike.to(args.device), masks_pred_dislike.to(args.device))\n",
    "        masks_pred,masks_context,a=generate(z1)\n",
    "    with open('./preference/like', 'wb') as f:\n",
    "        pickle.dump([a.unsqueeze(0),masks_pred,masks_context], f)\n",
    "        \n",
    "    #step4 finetune model based on user preference: sample-wise, we will get a new JEPA\n",
    "    epochs = 1\n",
    "    log_path= \"./preference/\"\n",
    "    load_checkpoint='./preference/jepa-latest.pth.tar'\n",
    "    like_path='./preference/like'\n",
    "    dislike_path='./preference/dislike'\n",
    "    command = f\"torchrun --nproc_per_node=1 train_personalization_sample.py --load_checkpoint {load_checkpoint} --epochs {epochs} --log_folder {log_path} --like_path {like_path} --dislike_path {dislike_path}\"\n",
    "    process = subprocess.run(command.split(), capture_output=True, text=True)\n",
    "    # Output the results\n",
    "    print(process.stdout)\n",
    "    print(process.stderr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722c1ac7-7e86-48e4-b883-cfdd66e0cd11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81ce249-73dc-4e07-bfc3-233086e70934",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
